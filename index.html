<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>StreamBWE</title>

  <!-- Fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <!-- Bulma + Icons (CDN, 不需要你再放本地文件) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <!-- Your styles -->
  <link rel="stylesheet" href="css/index.css" />
</head>

<body>
  <!-- Header -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              Dual-Modal Streaming Neural Speech Bandwidth Extension with Articulation-aware Lip Video Assistance
            </h1>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><a>Anonymous ICME submission</a>,</span>
              <!-- <span class="author-block"><a>Yuan Tian</a>,</span>
              <span class="author-block"><a href="https://faculty.ustc.edu.cn/aiyang/zh_CN/index.htm" target="_blank" rel="noreferrer">Yang Ai</a>,</span>
              <span class="author-block"><a>Hui-Peng Du</a>,</span>
              <span class="author-block"><a href="https://yxlu-0102.github.io/" target="_blank" rel="noreferrer">Ye-Xin Lu</a>,</span>
              <span class="author-block"><a href="http://staff.ustc.edu.cn/~zhling" target="_blank" rel="noreferrer">Zhen-Hua Ling</a></span> -->
            </div>

            <!-- 你要加 Code/Paper 按钮，取消注释并换链接即可 -->
            <!--
            <div class="buttons is-centered mt-4">
              <a class="button is-dark is-rounded" href="YOUR_GITHUB" target="_blank" rel="noreferrer">
                <span class="icon"><i class="fab fa-github"></i></span><span>Code</span>
              </a>
              <a class="button is-dark is-rounded" href="YOUR_ARXIV" target="_blank" rel="noreferrer">
                <span class="icon"><i class="ai ai-arxiv"></i></span><span>Paper</span>
              </a>
            </div>
            -->
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p id="abstract">
              Speech bandwidth extension (BWE) seeks to restore the high-frequency components lost in narrowband speech. 
              However, when the extension ratio is large, it often results in compromised quality and intelligibility. 
              Furthermore, many existing approaches neglect the essential requirement for low-latency streaming, thereby limiting their practical applicability in real-time contexts.
              Therefore, this paper proposes DMSBWE, a Dual-Modal Streaming neural speech BWE model, with lip video injected to improve the articulation of the extended speech.
              For the speech modality, a streaming causal convolutional network is employed to extend the real-valued, compact, and invertible modified discrete cosine transform (MDCT) spectrum. 
              For the visual modality, a streaming causal analysis-synthesis network is designed to extract articulation features from lip video. 
              To handle scenarios with or without lip video during inference, articulation features are integrated into the speech BWE stream, with explicit incorporation through global feature concatenation when visual input is available, and implicit integration via feature knowledge distillation when it is absent.
              Experimental results show that, with a large extension from 4 kHz to 48 kHz, the proposed DMSBWE improves ViSQOL by 0.25 and reduces WER by nearly 6%, compared to speech-only unimodal BWE, while maintaining ultra-low latency of 0.83 ms, confirming that articulation features extracted from lip video effectively enhance the quality and intelligibility of the extended speech.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Samples (auto-rendered) -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="content">
        <div class="sample-toolbar">
          <div class="field">
            <label class="label">Quick Jump</label>
            <div class="control">
              <div class="select is-fullwidth">
                <select id="jumpSelect"></select>
              </div>
            </div>
          </div>

          <div class="field">
            <label class="label">Layout</label>
            <div class="control">
              <div class="buttons">
                <button class="button is-small" id="btnDense">Dense</button>
                <button class="button is-small" id="btnComfort">Comfort</button>
              </div>
            </div>
          </div>
        </div>

        <div id="samplesRoot"></div>
      </div>
    </div>
  </section>

  <footer class="footer">
    <div class="content has-text-centered">
      <p class="is-size-7">
        © StreamBWE Demo Page. Static site generated by <code>js/data.js</code>.
      </p>
    </div>
  </footer>

  <!-- Data + Renderer -->
  <script src="js/data.js"></script>
  <script src="js/index.js"></script>
</body>
</html>
